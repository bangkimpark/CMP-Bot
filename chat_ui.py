import streamlit as st 
import markdown 
import io 
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer 
from reportlab.lib.styles import getSampleStyleSheet 
from reportlab.lib.pagesizes import A4 
from reportlab.pdfbase import pdfmetrics 
from reportlab.pdfbase.ttfonts import TTFont 
from reportlab.lib.styles import ParagraphStyle
from services.llm_service import LLMService
# ---------------------- # ì„¤ì • # ---------------------- 
# 
def setup_page(): 
    st.set_page_config(page_title="NHN Chat UI", page_icon="ğŸ¤–") 
    st.title("ğŸ¤– NHN CHAT BOT") 

#---------------------- # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” # ---------------------- 
def init_session_state(): 
    if "selected_mode" not in st.session_state: 
        st.session_state.selected_mode = None 

    if "messages" not in st.session_state: 
        st.session_state.messages = { "mode_a": [], "mode_b": [] } 
                
# ---------------------- # ìƒë‹¨ ë²„íŠ¼ ì˜ì—­ # ---------------------- 
def render_mode_selector(): 
    col1, col2 = st.columns(2) 
    with col1: 
        if st.button("ES ì¿¼ë¦¬ ìƒì„±"): 
            st.session_state.selected_mode = "mode_a" 
    with col2: 
        if st.button("ë¦¬ì†ŒìŠ¤ ë³´ê³ ì„œ ìƒì„±"): 
            st.session_state.selected_mode = "mode_b" 
    st.divider() 

def get_initial_message(mode: str) -> str: 
    if mode == "mode_a": 
        return ( 
            "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹\n" "Elasticsearch ì¿¼ë¦¬ ìƒì„± ë´‡ì…ë‹ˆë‹¤.\n\n" "- ìì—°ì–´ë¡œ ì›í•˜ëŠ” ì¡°ê±´ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”\n" "- ì˜ˆ: ìµœê·¼ 10ë¶„ê°„ ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ì´ 1GB ë¯¸ë§Œì¸ vm ì¡°íšŒ" ) 
    elif mode == "mode_b": return ( "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹\n" "ë¦¬ì†ŒìŠ¤/ìš´ì˜ ë³´ê³ ì„œ ìƒì„± ë´‡ì…ë‹ˆë‹¤.\n\n" "- ë³´ê³ ì„œ ëª©ì ê³¼ ë²”ìœ„ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”\n" "- ì˜ˆ: ìš´ì˜ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ í˜„í™© ë³´ê³ ì„œ" ) 
    return "" 

def markdown_to_pdf(markdown_text: str) -> bytes: 
    pdfmetrics.registerFont( TTFont("NotoSansKR", "fonts/NotoSansKR-Regular.ttf") ) 
    
    buffer = io.BytesIO() 
    
    doc = SimpleDocTemplate( buffer, pagesize=A4, rightMargin=40, leftMargin=40, topMargin=40, bottomMargin=40, ) 
    styles = getSampleStyleSheet() 
    
    # í•œê¸€ í°íŠ¸ ìŠ¤íƒ€ì¼ ì •ì˜ 
    styles.add( ParagraphStyle( name="Korean", fontName="NotoSansKR", fontSize=10, leading=14, ) ) 
    story = [] 
    # Markdown â†’ HTML 
    html = markdown.markdown(markdown_text) 
    # HTMLì„ ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ 
    for line in html.split("\n"): 
        if line.strip(): 
            story.append(Paragraph(line, styles["Korean"])) 
            story.append(Spacer(1, 12)) 
    doc.build(story) 
    buffer.seek(0) 
    return buffer.read() 

# ---------------------- # ì‘ë‹µ ìƒì„± ë¡œì§ (ë‚˜ì¤‘ì— LLM ì—°ê²° ì§€ì ) # ---------------------- 
def generate_response(mode: str, prompt: str) -> str: 
    if mode == "mode_a": # ë‹µë³€ ìƒ˜í”Œ (ì¶”í›„ llm ë‹µë³€ìœ¼ë¡œ ë³€ê²½) 
        query = """ 
        # ```json
        # GET nhn-monitoring_api-realtime-*/_search
        # {
        #     "size": 0,
        #     "query": {
        #         "bool": {
        #             "filter": [
        #                 { "term": { "workload_type": "vm" } },
        #                 { "term": { "data_name": "memory_available" } },
        #                 {
        #                     "range": {
        #                         "@timestamp": {
        #                             "gte": "now-10m",
        #                             "lte": "now"
        #                         }
        #                     }
        #                 },
        #                 {
        #                     "range": {
        #                         "data_value": {
        #                             "lt": 1073741824
        #                         }
        #                     }
        #                 }
        #             ]
        #         }
        #     },
        #     "aggs": {
        #         "by_vm": {
        #             "terms": {
        #                 "field": "vm_id",
        #                 "size": 100
        #             },
        #             "aggs": {
        #                 "latest": {
        #                     "top_hits": {
        #                         "size": 1,
        #                         "sort": [
        #                             { "@timestamp": { "order": "desc" } }
        #                         ],
        #                         "_source": {
        #                             "includes": [
        #                                 "vm_name",
        #                                 "instance_ip",
        #                                 "cluster_id",
        #                                 "data_value",
        #                                 "@timestamp"
        #                             ]
        #                         }
        #                     }
        #                 }
        #             }
        #         }
        #     }
        # }       
        # """
        answer=LLMService()
        return f"{answer.test_ask(query)}"
    elif mode == "mode_b":
        # ë‹µë³€ ìƒ˜í”Œ (ì¶”í›„ llm ë‹µë³€ìœ¼ë¡œ ë³€ê²½)
        answer = """
            # ìš´ì˜ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ í˜„í™© ë³´ê³ ì„œ

            ## 1. ê°œìš”
            ë³¸ ë³´ê³ ì„œëŠ” ëª¨ë‹ˆí„°ë§ APIë¥¼ í†µí•´ ìˆ˜ì§‘ëœ VM ë¦¬ì†ŒìŠ¤ ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ
            ìš´ì˜ í™˜ê²½ì˜ ì•ˆì •ì„±ì„ ì ê²€í•˜ê¸° ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

            ë³¸ ë³´ê³ ì„œëŠ” ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ
            ë¦¬ì†ŒìŠ¤ ì„ê³„ ìƒíƒœ ì—¬ë¶€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

            ---

            ## 2. ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ë° ê¸°ì¤€

            - ëŒ€ìƒ ë¦¬ì†ŒìŠ¤: ê°€ìƒë¨¸ì‹ (VM)
            - ë°ì´í„° ì¶œì²˜: nhn-monitoring API (realtime)
            - ì£¼ìš” ì§€í‘œ: ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ (`memory_available`)
            - ì¸¡ì • ë‹¨ìœ„: Byte
            - ì ê²€ ê¸°ì¤€:
            - ì •ìƒ: 1GB ì´ìƒ
            - ì£¼ì˜: 512MB ~ 1GB
            - ìœ„í—˜: 512MB ë¯¸ë§Œ

            ---

            ## 3. VM ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ í˜„í™© ìš”ì•½

            | VM ì´ë¦„ | VM ID | IP ì£¼ì†Œ | í´ëŸ¬ìŠ¤í„° | ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰(GB) | ìƒíƒœ |
            |-------|------|--------|---------|------------------|------|
            | inje_instance2 | 0001e0d8-a0d2-473b-8b19-21e66a517586 | 10.1.59.162 | v002 | 3.23 | ì •ìƒ |
            | example_vm01 | xxxx | 10.1.59.163 | v002 | 0.78 | ì£¼ì˜ |
            | example_vm02 | yyyy | 10.1.59.164 | v003 | 0.42 | ìœ„í—˜ |

            ---

            ## 4. ì´ìƒ ì§•í›„ ë° ë¶„ì„

            - ì¼ë¶€ VMì—ì„œ ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ í™•ì¸ë¨
            - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ì‹œì ê³¼ íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì´ë ¥ ê°„ ìƒê´€ ê°€ëŠ¥ì„± ì¡´ì¬
            - ìœ„í—˜ ìƒíƒœ VMì€ ë‹¨ê¸° ë‚´ ì¥ì•  ë°œìƒ ê°€ëŠ¥ì„± ì¡´ì¬

            ---

            ## 5. ì¡°ì¹˜ ë° ê¶Œê³  ì‚¬í•­

            - ìœ„í—˜ ìƒíƒœ VM:
            - í”„ë¡œì„¸ìŠ¤ ì ê²€ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒìœ„ í”„ë¡œì„¸ìŠ¤ í™•ì¸
            - í•„ìš” ì‹œ VM ì¬ê¸°ë™ ë˜ëŠ” ìŠ¤ì¼€ì¼ ì—… ê²€í† 
            - ì£¼ì˜ ìƒíƒœ VM:
            - 24ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°•í™”
            - ì„ê³„ì¹˜ í•˜í–¥ ì—¬ë¶€ ê²€í† 

            ---

            ## 6. ê²°ë¡ 
            ëª¨ë‹ˆí„°ë§ API ê¸°ë°˜ ì‹¤ì‹œê°„ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ì€
            ìš´ì˜ ì•ˆì •ì„± í™•ë³´ì— ì¤‘ìš”í•œ ì§€í‘œë¡œ í™œìš© ê°€ëŠ¥í•˜ë©°,
            ì§€ì†ì ì¸ ì„ê³„ì¹˜ ê´€ë¦¬ì™€ ì„ ì œì  ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.

            ---
            ì‘ì„±ì¼: 2026-01-28

            """
        return f"{answer}"
    return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œì…ë‹ˆë‹¤."


# ----------------------
# ì±„íŒ… UI
# ----------------------
def render_chat(mode: str):
    # ìµœì´ˆ ì§„ì… ì‹œ assistant ì•ˆë‚´ ë©”ì‹œì§€
    if len(st.session_state.messages[mode]) == 0:
        initial_msg = get_initial_message(mode)
        st.session_state.messages[mode].append({
            "role": "assistant",
            "content": initial_msg
        })
    for msg in st.session_state.messages[mode]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    if not prompt:
        return

    # user message
    st.session_state.messages[mode].append({
        "role": "user",
        "content": prompt
    })
    with st.chat_message("user"):
        st.markdown(prompt)

    # assistant message
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤– ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = generate_response(mode, prompt)
            st.markdown(response)

    st.session_state.messages[mode].append({
        "role": "assistant",
        "content": response
    })


    # ë³´ê³ ì„œ ëª¨ë“œì¼ ê²½ìš° PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    if mode == "mode_b":
        last_msg = st.session_state.messages[mode][-1]

        if last_msg["role"] == "assistant":
            pdf_bytes = markdown_to_pdf(last_msg["content"])

            st.download_button(
                label="ğŸ“„ ë³´ê³ ì„œ PDF ë‹¤ìš´ë¡œë“œ",
                data=pdf_bytes,
                file_name="siem_report.pdf",
                mime="application/pdf"
            )


# ----------------------
# main
# ----------------------
def main():
    setup_page()
    init_session_state()
    render_mode_selector()

    if st.session_state.selected_mode:
        render_chat(st.session_state.selected_mode)
    else:
        st.info("â¬†ï¸ ìœ„ì—ì„œ ê¸°ëŠ¥ì„ ì„ íƒí•˜ë©´ ì±„íŒ…ì´ ì‹œì‘ë©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()